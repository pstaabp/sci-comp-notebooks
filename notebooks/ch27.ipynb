{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Chapter 27: Optimization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This Chapter covers how to find the maximum or minimum of a function. We'll start with simple functions of 1 variable and move to functions of 2 variables. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "using Plots, Revise"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We'll start with a simple parabola:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f(x) = (x-1)^2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot(f,-0.25,2.25,legend=false)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "includet(\"../julia-files/Rootfinding.jl\")\n",
    "using .Rootfinding, ForwardDiff, LinearAlgebra"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To find the minimum, we'll seach for where the derivative is 0.  Instead of taking the derivative, though, we'll use the automatic differentiation in the `ForwardDiff` package."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "newton(x->ForwardDiff.derivative(f,x),0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Well that was fast.  The reason it was, however was that the derivative was a linear function. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f2(x) = sin(x)^2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot(f2,-1,8)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From the plot above, there are a lot of local minima.  We'll try to find the one new 3.5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "newton(x->ForwardDiff.derivative(f2,x),3.5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Optimizing functions of more that one variable"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's examine a function of two variables.  The following is a circular paraboloid, which you can think of as a parabola that is "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "g(x::Vector) = x[1]^2+x[2]^2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xrange = LinRange(-2,2,101)\n",
    "yrange = LinRange(-2,2,101)\n",
    "surface(xrange,yrange,(x,y)->g([x,y]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following is a contour plot of the same:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xrange = LinRange(-2,2,251)\n",
    "yrange = LinRange(-2,2,251)\n",
    "contour(xrange,yrange,(x,y)->g([x,y]), aspect_ratio = :equal, fill=true)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ForwardDiff.gradient(g,[0.5,1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We're going to look at the steepest descent method which take a point somewhere and moving opposite the gradient, which is the direction of steepest descent.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "function gradientDescent(f::Function,x₀::Vector; γ = 0.25, max_steps = 100)\n",
    "  local steps = 0\n",
    "  local ∇f₀ = [1,1] # initialize it to get into while loop\n",
    "  while norm(∇f₀)> 1e-8 && steps < max_steps\n",
    "    ∇f₀ = ForwardDiff.gradient(f,x₀)\n",
    "    x₀ -= γ*∇f₀\n",
    "    steps += 1\n",
    "  end\n",
    "  steps < max_steps || throw(ErrorException(\"The number of steps has exceeded $max_steps\"))\n",
    "  @show steps\n",
    "  x₀\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gradientDescent(g,[0.5,1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's look at another function (this is famous in optimization circles):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rose(x::Vector) = (1-x[1])^2+50*(x[2]-x[1]^2)^2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xrange = LinRange(-1,3,251)\n",
    "yrange = LinRange(-1,4,251)\n",
    "surface(xrange,yrange,(x,y)->rose([x,y]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "contour(xrange,yrange,(x,y)->rose([x,y]),levels=[1,2,3,10,50,100,200,500,1000,10_000], fill=true)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ForwardDiff.gradient(rose,[-0.5,0.5])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If we run the gradient descent method on the rose function:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gradientDescent(rose,[-0.5,0.5])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What's going on with this?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x0 = [-0.5,0.5]\n",
    "∇f0 = ForwardDiff.gradient(rose,x0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x1 = x0 - 0.25∇f0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "∇f1 = ForwardDiff.gradient(rose,x1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's play with the $\\gamma$ parameter."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From the example above, we eventually found the solution, but had to fiddle with the $\\gamma$ parameter.  The following uses an adaptive value of\n",
    "$$\\gamma = \\frac{|(\\vec{x}_1-\\vec{x}_0)\\cdot (\\nabla f(\\vec{x}_1) - \\nabla f(\\vec{x}_0))|}{||\\nabla f(\\vec{x}_1) - \\nabla f(\\vec{x}_0))||^2}$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "function gradientDescentBB(f::Function,x₀::Vector; max_steps = 100)\n",
    "  local steps = 0\n",
    "  local ∇f₀ = ForwardDiff.gradient(f,x₀)\n",
    "  local x₁ = x₀ - 0.25 * ∇f₀ # need to start with a value for x₁\n",
    "  while norm(∇f₀)> 1e-4 && steps < max_steps\n",
    "    ∇f₁ = ForwardDiff.gradient(f,x₁)\n",
    "    Δ∇f = ∇f₁-∇f₀\n",
    "    x₂ = x₁ - abs(dot(x₁-x₀,Δ∇f))/norm(Δ∇f)^2*∇f₁\n",
    "    x₀ = x₁\n",
    "    x₁ = x₂\n",
    "    ∇f₀ = ∇f₁\n",
    "    steps += 1\n",
    "  end\n",
    "  @show steps\n",
    "  steps < max_steps || throw(ErrorException(\"The number of steps has exceeded $max_steps\"))\n",
    "  x₁\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gradientDescentBB(rose,[-0.5,0.5])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Exercise\n",
    "- Produce a contour plot of the function $f(x,y) = \\sin(0.5x^2-0.25y^2+2)*\\cos(x+y)$ on the domain $[0,\\pi]\\times[0,\\pi]$\n",
    "- See if you can find the minimum of $f(x,y)$ using gradient descent. \n",
    "- See if you can find the minimum of $f(x,y)$ using the  Barzilai–Borwein gradient descent method.\n",
    "- Find the maximum of $f(x,y)$ by minimizing $-f(x,y)$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Using the JuMP package"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "using JuMP, Ipopt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Model(Ipopt.Optimizer)\n",
    "set_optimizer_attribute(model,\"print_level\",5) # this can be level 1 through 12.  1 minimal.\n",
    "@variable(model, x, start = 0.0)\n",
    "@variable(model, y, start = 0.0)\n",
    "\n",
    "@NLobjective(model, Min, (1 - x)^2 + 100 * (y - x^2)^2)\n",
    "\n",
    "optimize!(model)\n",
    "@show value(x),value(y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise\n",
    "Use the JuMP package to find both the minimum and maximum of the above function $f(x,y)$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Minimizing a function of more that 3 variables"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is a bit difficult to visualize in many cases.  There are 3D contour plots, but generally are hard to read.  We'll use some of the above techniques to find the minimum of \n",
    "$$ h(x,y,z) = \\sin(x+y^2-\\pi z)\\cos(2x+3z^3)$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "h(x) = sin(x[1]-x[2]^2-pi*x[3])*cos(2*x[1]+3*x[3]^3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "min_h = gradientDescentBB(h,[1,1,1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "h(min_h)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Model(Ipopt.Optimizer)\n",
    "@variable(model, x, start = 1.0)\n",
    "@variable(model, y, start = 1.0)\n",
    "@variable(model, z, start=1.0)\n",
    "\n",
    "@NLobjective(model, Min, sin(x-y^2-pi*z)*cos(2*x+3*z^3))\n",
    "\n",
    "optimize!(model)\n",
    "@show value(x),value(y),value(z)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Julia 1.8.2",
   "language": "julia",
   "name": "julia-1.8"
  },
  "language_info": {
   "file_extension": ".jl",
   "mimetype": "application/julia",
   "name": "julia",
   "version": "1.8.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
